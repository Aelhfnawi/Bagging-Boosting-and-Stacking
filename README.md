Ensemble Learning: Bagging, Boosting, and Stacking

This repository contains a Jupyter Notebook that explores ensemble learning techniques in machine learning. Ensemble learning combines multiple models to improve prediction accuracy, robustness, and generalization compared to single models.

📘 Contents

The notebook covers:

Bagging (Bootstrap Aggregating)

Concept and process

Example: Random Forest

Boosting

Concept and process

Examples: AdaBoost, Gradient Boosting, XGBoost

Stacking

Concept and process

Example: Stacking Classifier

Voting Classifier

Hard vs. Soft voting

Summary of all techniques

⚙️ Key Algorithms and Parameters

Random Forest

n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features

AdaBoost

estimator, n_estimators, learning_rate

📊 Applications

Reduce overfitting (Bagging)

Improve accuracy on complex datasets (Boosting)

Leverage multiple models’ strengths (Stacking)

🧑‍💻 Author

Created by Ahmed Hesham Elhfnawi.

If you find this useful, feel free to ⭐ the repo!
